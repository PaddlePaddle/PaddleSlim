{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/framework.py:507: UserWarning: PaddlePaddle version 2.3.0 or higher is required, but 0.0.0 installed, Maybe you are using a develop version, please make sure the version is good with your code.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import paddle\n",
    "import numpy as np\n",
    "import types\n",
    "import paddleslim\n",
    "import copy\n",
    "\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import Dataset\n",
    "from paddle.nn import Conv2D, Linear, ReLU, Sequential\n",
    "from paddle.quantization import QAT, QuantConfig\n",
    "from paddle.quantization.quanters import FakeQuanterWithAbsMaxObserver\n",
    "from paddle.quantization.quanters.abs_max import (\n",
    "    FakeQuanterWithAbsMaxObserverLayer,\n",
    ")\n",
    "from paddle.vision.models import resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "\n",
    "# def _visit(model):\n",
    "    \n",
    "#     for _name, _layer in model.named_sublayers():\n",
    "#         print(f\"name: {_name}; layer type: {type(_layer)}; layer full name: {_layer.full_name()}\")\n",
    "#         _visit(_layer)\n",
    "        \n",
    "# _visit(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _find_parent_layer_and_sub_name(model, layer):\n",
    "    for _name, _sub_layer in model.named_sublayers():\n",
    "        if layer.full_name() == _sub_layer.full_name():\n",
    "            return model, _name\n",
    "        else:\n",
    "            result = _find_parent_layer_and_sub_name(_sub_layer, layer)\n",
    "            if result is not None:\n",
    "                return result\n",
    "\n",
    "def replace_layer(model, source, target):\n",
    "    \n",
    "    parent_layer, sub_name = _find_parent_layer_and_sub_name(model, source)\n",
    "    parent_layer._sub_layers[sub_name] = target\n",
    "    setattr(parent_layer, sub_name, target)\n",
    "    #print(f\"replace {sub_name} of {parent_layer.full_name()} from {type(source)} to {type(target)}\")\n",
    "\n",
    "class ConvBNWrapper(paddle.nn.Layer):\n",
    "    def __init__(self, conv, bn):\n",
    "        super(ConvBNWrapper, self).__init__()\n",
    "        self._conv = conv\n",
    "        self._bn = bn\n",
    "    def forward(self, inputs):\n",
    "        return self._bn(self._conv(inputs))\n",
    "    \n",
    "    \n",
    "\n",
    "class Constraint():\n",
    "    \"\"\"在量化训练或离线量化过程中，需要遵循的约束。可以是且不限于以下几种约束：\n",
    "    1. Operators Fusion: 将多个Operators当做一个融合的Operator，只量化融合Operator的输入和输出\n",
    "    2. 多个Tensors的量化相互影响：比如多个Tensors量化参数需要保持一致，或需要满足更复杂的要求\n",
    "    3. 统计量化参数的过程与常规的forward流程不一样，需要特殊处理\n",
    "    \"\"\"\n",
    "    \n",
    "    def apply(self, model, qconfig):\n",
    "        \"\"\"将约束应用到目标模型上，并更新量化配置信息。应该在量化训练和离线量化校准操作前执行该方法。\n",
    "        该方法会直接inplace地对model和qconfig进行操作。\n",
    "        该方法为抽象方法，所有继承Constraint的子类都应该实现该方法。\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    \n",
    "class FusionConstraint(Constraint):\n",
    "    \"\"\"Define some functoins used to fuse operators.\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace_layer(model, source, target):\n",
    "    \n",
    "        parent_layer, sub_name = _find_parent_layer_and_sub_name(model, source)\n",
    "        parent_layer._sub_layers[sub_name] = target\n",
    "        setattr(parent_layer, sub_name, target)\n",
    "    \n",
    "    def _fuse_ops(self, model, fused_layer_type, pair):\n",
    "        fused_op = fused_layer_type(*pair)\n",
    "        for _layer in pair[1:]:\n",
    "            replace_layer(model, _layer, paddle.nn.Identity())\n",
    "            del self._config._layer2config[_layer]\n",
    "            \n",
    "        replace_layer(model, pair[0], fused_op)\n",
    "        self._config._layer2config[fused_op] = self._config._layer2config[pair[0]]\n",
    "    \n",
    "class FreezedConvBNConstraint(FusionConstraint):\n",
    "    \n",
    "    \n",
    "class TIConvBiasConstraint(FusionConstraint):\n",
    "    \n",
    "\n",
    "class TIQAT(QAT):\n",
    "    def __init__(self, q_config):\n",
    "        super(TIQAT, self).__init__(q_config)\n",
    "\n",
    "\n",
    "    def _analysis_and_fuse_ops(self, model, inputs):\n",
    "        assert inputs is not None\n",
    "        tracer = paddleslim.core.GraphTracer(model)\n",
    "        tracer(inputs)\n",
    "        graph = tracer.graph\n",
    "\n",
    "        if self._config.need_fuse_conv_bn:\n",
    "            conv_bn_pairs = graph.find_conv_bn()\n",
    "            for pair in conv_bn_pairs:\n",
    "                pair = [node._layer for node in pair]\n",
    "                self._fuse_ops(model, ConvBNWrapper, pair)\n",
    "                self._config.add_qat_layer_mapping(ConvBNWrapper, paddleslim.quant.nn.QuantedConv2DBatchNorm)\n",
    "\n",
    "    def _fuse_ops(self, model, fused_layer_type, pair):\n",
    "        fused_op = fused_layer_type(*pair)\n",
    "        for _layer in pair[1:]:\n",
    "            replace_layer(model, _layer, paddle.nn.Identity())\n",
    "            del self._config._layer2config[_layer]\n",
    "            \n",
    "        replace_layer(model, pair[0], fused_op)\n",
    "        self._config._layer2config[fused_op] = self._config._layer2config[pair[0]]\n",
    "\n",
    "    def quantize(self, model: paddle.nn.Layer, inplace=False, inputs=None):\n",
    "        _model = model if inplace else copy.deepcopy(model)\n",
    "        self._config._specify(_model)\n",
    "        self._analysis_and_fuse_ops(_model, inputs)\n",
    "        self._convert_to_quant_layers(_model, self._config)\n",
    "        self._insert_activation_observers(_model, self._config)\n",
    "        return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/paddle/nn/layer/norm.py:771: UserWarning: When training, we now always track global mean and variance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "self._qat_layer_mapping: {<class 'paddle.nn.quant.stub.Stub'>: <class 'paddle.nn.quant.stub.QuanterStub'>, <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>, <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>, <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>}\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class '__main__.ConvBNWrapper'>: <class 'paddleslim.quant.nn.conv_bn.QuantedConv2DBatchNorm'>\n",
      "get qat layer of <class 'paddle.nn.layer.conv.Conv2D'>: <class 'paddle.nn.quant.qat.conv.QuantedConv2D'>\n",
      "get qat layer of <class 'paddle.nn.layer.common.Linear'>: <class 'paddle.nn.quant.qat.linear.QuantedLinear'>\n"
     ]
    }
   ],
   "source": [
    "quanter = FakeQuanterWithAbsMaxObserver(moving_rate=0.9)\n",
    "q_config = QuantConfig(activation=quanter, weight=quanter)\n",
    "q_config.need_fuse_conv_bn = True\n",
    "qat = TIQAT(q_config)\n",
    "x = paddle.rand([1, 3, 224, 224])\n",
    "quant_model = qat.quantize(model, inputs=x)\n",
    "#paddle.jit.save(quant_model, \"./qat_model\", input_spec=[x])\n",
    "# convert_model = qat.convert(quant_model)\n",
    "# paddle.jit.save(convert_model, \"./fp_infer\", input_spec=[x])\n",
    "# out = quant_model(x)\n",
    "# out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): QuantedConv2DBatchNorm(\n",
      "    (bn): BatchNorm(\n",
      "      (_bn): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "    (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): ObserveWrapper(\n",
      "    (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "    (_observed): ReLU()\n",
      "  )\n",
      "  (maxpool): ObserveWrapper(\n",
      "    (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "    (_observed): MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantedConv2DBatchNorm(\n",
      "          (bn): BatchNorm(\n",
      "            (_bn): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "          )\n",
      "          (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "          (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantedConv2D(\n",
      "          (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "          (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        )\n",
      "        (1): ObserveWrapper(\n",
      "          (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "          (_observed): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantedConv2DBatchNorm(\n",
      "          (bn): BatchNorm(\n",
      "            (_bn): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "          )\n",
      "          (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "          (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantedConv2DBatchNorm(\n",
      "        (bn): BatchNorm(\n",
      "          (_bn): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "        )\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): ReLU()\n",
      "      )\n",
      "      (conv2): QuantedConv2D(\n",
      "        (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "      )\n",
      "      (bn2): ObserveWrapper(\n",
      "        (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "        (_observed): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): ObserveWrapper(\n",
      "    (_observer): FakeQuanterWithAbsMaxObserverLayer()\n",
      "    (_observed): AdaptiveAvgPool2D(output_size=(1, 1))\n",
      "  )\n",
      "  (fc): QuantedLinear(\n",
      "    (weight_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "    (activation_quanter): FakeQuanterWithAbsMaxObserverLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导出ONNX格式模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./ \\\n",
    "            --model_filename fp_infer.pdmodel \\\n",
    "            --params_filename fp_infer.pdiparams \\\n",
    "            --save_file fp_infer.onnx \\\n",
    "            --opset_version 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddleslim\n",
    "print(dir(paddleslim.quant))\n",
    "print(paddleslim.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
