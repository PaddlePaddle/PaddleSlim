# 版本更新信息

## 最新版本信息

### v1.1.0(05/2020)

- 量化
  - 增加无校准数据训练后量化方法，int16精度无损，int8精度损失低于0.1%。
  - 增强量化功能，完善量化OP的输出scale信息，支持CPU预测端全面适配量化模型。
- 剪裁
  - 新增FPGM和BN scale两种剪裁策略, 在MobileNetV3-YOLOV3-COCO任务上，同等压缩率下精度提升0.6%。
  - 新增自定义剪裁策略接口，方便开发者快速新增压缩策略。
  - 剪裁功能添加对新增Operator的默认处理逻辑，扩展支持剪裁更多复杂网络。
- NAS
  - 新增DARTS系列搜索算法，并提供扩展接口，方便用户调研和实现新的模型结构搜索策略。
  - 模型结构搜索添加早停机制，提升搜索功能易用性。
  - 新增一种基于强化学习的模型结构搜索策略，并提供扩展接口，为用户调研实现新策略提供参考。


## 历史版本信息

### v1.0.1

  - 拆分PaddleSlim为独立repo。
  - 重构裁剪、量化、蒸馏、搜索接口，对用户开放底层接口。
      - 量化: 
          - 新增基于KL散度的离线量化功能，支持对Embedding层量化。
          - 新增对FC的QAT MKL-DNN量化策略支持 
          - 新增PostTrainingQuantization，完整实现训练后量化功能：支持量化30种OP，支持灵活设置需要量化的OP，生成统一格式的量化模型，具有耗时短、易用性强、精度损失较小的优点。
          - 量化训练支持设定需要量化的OP类型。
      - 裁剪: 重构剪裁实现，方便扩展支持更多类型的网络。
      - 搜索:
          - 支持SA搜索，增加更多的搜索空间，支持用户自定义搜索空间。
          - 新增one-shot搜索算法，搜索速度比上个版本快20倍。
  - 新增大规模可扩展知识蒸馏框架 Pantheon
      - student 与 teacher 、teacher与 teacher 模型之间充分解耦，可分别独立运行在不同的物理设备上，便于充分利用计算资源；
      - 支持 teacher 模型的单节点多设备大规模预测，在 BERT 等模型上测试加速比达到线性；
      - 用 TCP/IP 协议实现在线蒸馏模式的通信，支持在同一网络环境下，运行在任意两个物理设备上的 teacher 模型和 student 模型之间进行知识传输；
      - 统一在线和离线两种蒸馏模式的 API 接口，不同的 teacher 模型可以工作在不同的模式下；
      - 在 student 端自动完成知识的归并与知识数据的 batch 重组，便于多 teacher 模型的知识融合。
  - 模型库:
      - 发布ResNet50、MobileNet模型的压缩benchmark
      - 打通检测库，并发布YOLOv3系列模型的压缩benchmark
      - 打通分割库，并发布Deepabv3+系列分割模型的压缩benchmark
  - 完善文档：
      - 补充API文档；新增入门教程和高级教程；增加ModelZoo文档，覆盖分类、检测、分割任务。所有文档包含中、英文。
