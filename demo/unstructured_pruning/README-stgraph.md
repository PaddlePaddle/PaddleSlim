# 非结构化稀疏 -- 静态图剪裁（包括按照阈值和比例剪裁两种模式）

## 简介

在模型压缩中，常见的稀疏方式为结构化和非结构化稀疏，前者在某个特定维度（特征通道、卷积核等等）上进行稀疏化操作；后者以每一个参数为单元进行稀疏化，所以更加依赖于硬件对稀疏后矩阵运算的加速能力。本目录即在PaddlePaddle和PaddleSlim框架下开发的非结构化稀疏算法，MobileNetV1在ImageNet上的稀疏化实验中，剪裁率55.19%，达到无损的表现。

## 版本要求
```bash
python3.5+
paddlepaddle>=2.0.0
paddleslim>=2.0.0
```

请参照github安装[paddlepaddle](https://github.com/PaddlePaddle/Paddle)和[paddleslim](https://github.com/PaddlePaddle/PaddleSlim)。

## 使用

训练前：
- 预训练模型下载，并放到某目录下，通过train.py中的--pretrained_model设置。
- 训练数据下载后，可以通过重写../imagenet_reader.py文件，并在train.py文件中调用实现。
- 开发者可以通过重写UnstructurePruner.mask_parameters()和UnstructurePruner.update_threshold()来定义自己的非结构化稀疏策略（目前为剪裁掉绝对值小的parameters）。
- 开发可以通过重写UnstructurePruner.get_skip_params()来定义哪些参数在不能被剪裁。默认为所有的归一化层的参数不参与剪裁。

训练：
```bash
CUDA_VISIBLE_DEVICES=2,3 python3 train-dygraph.py --data imagenet --lr 0.1 --phase prune --pruning_mode ratio --ratio=0.5
```

推理：
```bash
CUDA_VISIBLE_DEVICES=0 python3 train.py --pretrained_model models/ --phase test --data imagenet
```

剪裁训练代码示例：
```python
# model definition
places = paddle.static.cuda_places()
place = places[0]
exe = paddle.static.Executor(place)
model = models.__dict__[args.model]()
out = model.net(input=image, class_dim=class_dim)
cost = paddle.nn.functional.loss.cross_entropy(input=out, label=label)
avg_cost = paddle.mean(x=cost)
acc_top1 = paddle.metric.accuracy(input=out, label=label, k=1)
acc_top5 = paddle.metric.accuracy(input=out, label=label, k=5)

val_program = paddle.static.default_main_program().clone(for_test=True)

opt, learning_rate = create_optimizer(args, step_per_epoch)
opt.minimize(avg_cost)

#STEP1: initialize the pruner and the 'threshold' placeholder
threshold = paddle.static.data(name='pruning_threshold', shape=[1], dtype='float32')
pruner = UnstructurePruner(paddle.static.default_main_program(), mode='ratio', threshold=threshold, ratio_value=0.5, place=place)

exe.run(paddle.static.default_startup_program())
paddle.fluid.io.load_vars(exe, args.pretrained_model)

for epoch in range(epochs):
    for batch_id, data in enumerate(train_loader):
        loss_n, acc_top1_n, acc_top5_n = exe.run(
            train_program,
            feed={
                "image": data[0].get('image'),
                "label": data[0].get('label'),
                "pruning_threshold": pruner.threshold_value
            },
            fetch_list=[avg_cost.name, acc_top1.name, acc_top5.name])  
        learning_rate.step()
        #STEP2: update the pruner's threshold given the updated parameters
        pruner.step()

    if epoch % args.test_period == 0:
        #STEP3: before evaluation during training, eliminate the non-zeros generated by opt.step(), which, however, the cached masks setting to be zeros.
        pruner.update_params()
        eval(epoch)

    if epoch % args.model_period == 0:
        # STEP4: same purpose as STEP3
        pruner.update_params()
        save(epoch)
```

剪裁后测试代码示例：
```python
# intialize the model instance in static mode
# load weights
print(UnstructurePruner.total_sparse(paddle.static.default_main_program())) #注意，total_sparse为静态方法(static method)，可以不创建实例(instance)直接调用，方便只做测试的写法。
test()
```

更多使用参数请参照如下，请按照实际数据集和GPU资源进行调整：
```bash
usage: train-stgraph.py [-h] [--batch_size BATCH_SIZE] [--use_gpu USE_GPU]
                        [--model MODEL] [--pretrained_model PRETRAINED_MODEL]
                        [--lr LR] [--lr_strategy LR_STRATEGY]
                        [--l2_decay L2_DECAY] [--momentum_rate MOMENTUM_RATE]
                        [--threshold_value THRESHOLD_VALUE]
                        [--pruning_mode PRUNING_MODE]
                        [--ratio_value RATIO_VALUE] [--num_epochs NUM_EPOCHS]
                        [--step_epochs STEP_EPOCHS [STEP_EPOCHS ...]]
                        [--data DATA] [--log_period LOG_PERIOD]
                        [--phase PHASE] [--test_period TEST_PERIOD]
                        [--model_path MODEL_PATH]
                        [--model_period MODEL_PERIOD]
                        [--resume_epoch RESUME_EPOCH]

optional arguments:
  -h, --help            show this help message and exit
  --batch_size BATCH_SIZE
                        Minibatch size. Default: 256.
  --use_gpu USE_GPU     Whether to use GPU or not. Default: True.
  --model MODEL         The target model. Default: MobileNet.
  --pretrained_model PRETRAINED_MODEL
                        Whether to use pretrained model. Default:
                        ../pretrained_model/MobileNetV1_pretrained.
  --lr LR               The learning rate used to fine-tune pruned model.
                        Default: 0.1.
  --lr_strategy LR_STRATEGY
                        The learning rate decay strategy. Default:
                        piecewise_decay.
  --l2_decay L2_DECAY   The l2_decay parameter. Default: 3e-05.
  --momentum_rate MOMENTUM_RATE
                        The value of momentum_rate. Default: 0.9.
  --threshold_value THRESHOLD_VALUE
                        The threshold to set zeros, the abs(weights) lower
                        than which will be zeros. Default: 1e-05.
  --pruning_mode PRUNING_MODE
                        the pruning mode: whether by ratio or by threshold.
                        Default: ratio.
  --ratio_value RATIO_VALUE
                        The threshold to set zeros, the abs(weights) lower
                        than which will be zeros. Default: 0.5.
  --num_epochs NUM_EPOCHS
                        The number of total epochs. Default: 120.
  --step_epochs STEP_EPOCHS [STEP_EPOCHS ...]
                        piecewise decay step
  --data DATA           Which data to use. 'mnist' or 'imagenet'. Default:
                        mnist.
  --log_period LOG_PERIOD
                        Log period in batches. Default: 100.
  --phase PHASE         Whether to train or test the pruned model. Default:
                        train.
  --test_period TEST_PERIOD
                        Test period in epoches. Default: 10.
  --model_path MODEL_PATH
                        The path to save model. Default: ./models.
  --model_period MODEL_PERIOD
                        The period to save model in epochs. Default: 10.
  --resume_epoch RESUME_EPOCH
                        The epoch to resume training. Default: 0.
```

## 实验结果 （刚开始在动态图代码验证，以下为静态图代码上的结果）

| 模型 | 数据集 | 压缩方法 | 压缩率| Top-1/Top-5 Acc | lr | threshold | epoch |
|:--:|:---:|:--:|:--:|:--:|:--:|:--:|:--:|
| MobileNetV1 | ImageNet | Baseline | - | 70.99%/89.68% | - | - | - |
| MobileNetV1 | ImageNet |   ratio  | -55.19% | 70.87%/89.80% (-0.12%/+0.12%) | 0.005 | - | 68 |
| YOLO v3     |  VOC     | - | - |76.24% | - | - | - |
| YOLO v3     |  VOC     |threshold | -41.35% | 75.29%（-0.95%） | 0.005 | 0.05 | 10w |
| YOLO v3     |  VOC     |threshold | -53.00% | 75.00%（-1.24%） | 0.005 | 0.075 | 10w |

## TODO

- [ ] 完成实验，验证动态图下的效果，并得到压缩模型。
- [ ] 扩充衡量parameter重要性的方法（目前仅为绝对值）。
