# Image classification INT8 model deployment and Inference on CPU

## Overview

This document describes the process of converting, deploying and executing the DNNL INT8 model using fake quantized model generated by PaddleSlim. On Casecade Lake machines (eg. Intel(R) Xeon(R) Gold 6271, 6248, X2XX etc), the INT8 model is 3-4 times faster than the FP32 model. On SkyLake machines (eg. Intel(R) Xeon(R) Gold 6148, 8180, X1XX etc.), the INT8 model is ~1.5 times faster than FP32 model.

The steps are as follows:
- Generate fake quantized model: Use PaddleSlim to generate fake quantized model with strategy: Quant-aware training or post-training. Note that the parameters of the quantized ops will be in the range of INT8, but the type remain float32.
- Converting fake quantized model to real INT8 model: Use the script to convert the quant model to DNNL INT8 model on the CPU.
- Deployment and inference on CPU: Deploy the demo on CPUs and run the inference.

## 1. Preparation

#### Install PaddleSlim

For PaddleSlim installation, please refer to [Paddle Installation Document](https://paddlepaddle.github.io/PaddleSlim/install.html)
```
git clone https://github.com/PaddlePaddle/PaddleSlim.git
cd PaddleSlim
python setup.py install
```
#### Use it in examples
In sample tests, import Paddle and PaddleSlim as follows:
```
import paddle
import paddle.fluid as fluid
import paddleslim as slim
import numpy as np
```

## 2. Use PaddleSlim to generate a fake quantized model

#### 2.1 Quant-aware training

To generate fake quantized model with quant-aware strategy, refer to [Quant-aware training tutorial](https://paddlepaddle.github.io/PaddleSlim/tutorials/quant_aware_demo/)

**The parameters during quant-aware training:**
- **quantize_op_types:** In PaddlePaddle, CPU supports quantizable ops: `depthwise_conv2d`, `mul`, `conv2d`, `matmul`, `transpose2`, `reshape2`, `pool2d`, `scale`, `concat`. However, when inserting fake quantize/dequantize op during training, you only need to insert fake quantize/dequantize ops before and after the first four ops, because the latter five ops: `transpose2`, `reshape2`, `pool2d`, `scale`, `concat`, their input and output scales can be obtained from the scales of other ops before and after, so setting `quantize_op_types` parameter `depthwise_conv2d`, `mul`, `conv2d`, `matmul` is enough.
- **Other parameters:** Please refer to [PaddleSlim quant_aware API](https://paddlepaddle.github.io/PaddleSlim/api/quantization_api/#quant_aware)

#### 2.2 Post-training quantization

To generate post-training fake quantized model, refer to [Offline post-training quantization tutorial](https://paddlepaddle.github.io/PaddleSlim/tutorials/quant_post_demo/#_1)

## 3. Convert the fake quantized model to DNNL INT8 model
In order to deploy on the CPU, we need to collect scales, remove the fake quantize/dequantize op, fuse some ops, and finally convert fake quantized ops to real INT8 ops (values in INT8 range, and datatype is int8 type too). Run the following script you can convert fake quantized model to DNNL INT8 model. The script locates [save_quant_model.py](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/contrib/slim/tests/save_quant_model.py). Copy the script to the directory where the demo is located: `/PATH_TO_PaddleSlim/demo/mkldnn_quant/quant_aware/` and run it as follows:
```
python save_quant_model.py --quant_model_path=/PATH/TO/SAVE/FLOAT32/quant/MODEL --int8_model_save_path=/PATH/TO/SAVE/INT8/MODEL
```

**Available options in the above command and their descriptions are as follows:**
- **quant_model_path:** input model path, required. A quant model for quantifying training output.
- **int8_model_save_path:** The final INT8 model output path after the quant model is optimized and quantized by DNNL.
- **ops_to_quantize:** A comma separated list of specified op types to be quantified. Optional, default is empty, empty means to quantify all quantiable ops. At present, for the image classification and natural language processing models listed in Benchmark, quantizing all quantiable ops can obtain the best accuracy and performance, so it is recommended that users not set this parameter.
- **--op_ids_to_skip:** A list of op id numbers separated by commas, optional. Default value is none. The op numbers in this list will not be quantified and adopt the FP32 type. To get the ID of a specific op, first run the script using the `--debug` option, and open the generated file `qat_int8_cpu_quantize_placement_pass.dot` to find the op that does not need to be quantified, and the ID number is in parentheses after the Op name.
- **--debug:** Generate models graph or not. Add this option to generate a series of *.dot files containing model drawings after each conversion step. For the description of DOT format, please refer to [DOT](https://graphviz.gitlab.io/_pages/doc/info/lang.html). To open the `*.dot` file, please use any Graphviz tool available on the system(such as the `xdot` tool on Linux or the `dot` tool on Windows. For Graphviz documentation, see [Graphviz](http://www. graphviz.org/documentation/).
  
- **Note:**
  - The DNNL supported quantizable ops are `conv2d`, `depthwise_conv2d`, `mul`, `fc`, `matmul`, `pool2d`, `reshape2`, `transpose2`, `scale`, `concat`.
  - If you set `--op_ids_to_skip`, you only need to pass in all the quantized op ID number you want to keep FP32 type.
  - Sometimes quantizing all ops does not necessarily result in optimal performance. For example: If an op is a single INT8 op, before and after the op are float32 op, then in order to quantify this op, you need to do quantize first, then run INT8 op, and then dequantize, which may lead to the final performance is not as good as keeping the op fp32 op. If the user has poor performance using the default settings, you can observe whether this model has a separate INT8 op, select different combinations of `ops_to_quantize`, or you can exclude some quantiable op IDs through `--op_ids_to_skip`, and run several times to get the most Good settings.

## 4. Inference

### 4.1 Data preprocessing
To deploy on the CPU, the data needs to be binary converted first. Run the script as follows to transform the complete ILSVRC2012 val data set. Use `--local` to convert user's own data. Run the following script in the directory where Paddle is located. The script is located on the official website at [full_ILSVRC2012_val_preprocess.py](https://github.com/PaddlePaddle/Paddle/blob/develop/paddle/fluid/inference/tests/api/full_ILSVRC2012_val_preprocess.py)
```
python Paddle/paddle/fluid/inference/tests/api/full_ILSVRC2012_val_preprocess.py --local --data_dir=/PATH/TO/USER/DATASET/ --output_file=/PATH/TO/SAVE/BINARY/FILE
```

**Available options in the above command and their descriptions are as follows:**
- No parameters are set. The script will download the ILSVRC2012_img_val data set and convert it into a binary file.
- **local:** is set to true, indicating that users will provide their own data
- **data_dir:** user's own data directory
- **label_list:** Picture path-Picture category list file, similar to `val_list.txt`
- **output_file:** The path of the generated binary file.
- **data_dim:** Pre-process the length and width of the picture. The default value is 224.

The user's own data set directory structure should be as follows
```
imagenet_user
├── val
│ ├── ILSVRC2012_val_00000001.jpg
│ ├── ILSVRC2012_val_00000002.jpg
| |── ...
└── val_list.txt
```
Among them, the content of val_list.txt should be as follows:
```
val/ILSVRC2012_val_00000001.jpg 0
val/ILSVRC2012_val_00000002.jpg 0
```

note:
-Why convert the data set into a binary file? Because the data preprocessing (resize, crop, etc.) in paddle is performed using the pythong.Image module, the trained model is also based on the image of Python preprocessing, but we found that Python testing performance overhead is very large, resulting in reduced inference performance. In order to obtain good performance, in the quantitative model inference stage, we decided to use C++ test, and C++ only supports libraries such as Open-CV, Paddle does not recommend the use of external libraries, so we use Python to preprocess the image and then put it into a binary file Read it in the C++ test. According to their own needs, users can change the C++ test to use the open-cv library to directly read the data and preprocess it, and the accuracy will not be greatly reduced. We also provide the python test `sample_tester.py` as a reference. Compared with the C++ test `sample_tester.cc`, users can see that the Python test has a greater performance overhead.

### 4.2 Deploy Inference demo

#### Deployment premise
- Users can view the local support commands by entering `lscpu` on the command line.
- On CPU servers that support `avx512_vnni`, INT8 has the highest accuracy.
- On CPU servers that support `avx512` (Casecade Lake, Model name: Intel(R) Xeon(R) Gold X2XX, such as 6271, 6248), the performance of INT8 is improved by 3~4 times of FP32 model.
- On CPU servers that do not support `avx512` but support `avx2` instructions (SkyLake, Model name: Intel(R) Xeon(R) Gold X1XX, such as 6148), the performance of INT8 is about 1.5 times that of FP32.

#### Prepare Paddle inference library

Users can compile the Paddle inference library from the source code or download the inference library directly.
-Users can compile the Paddle inference library from the Paddle source code, refer to [Compile from Source](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/advanced_guide/inference_deployment/inference/build_and_install_lib_cn.html#id12) , Use release/2.0 or later.

-Users can also download the published [inference Library] from the official website of Paddle (https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/advanced_guide/inference_deployment/inference/build_and_install_lib_cn.html). Please select `ubuntu14.04_cpu_avx_mkl` latest release or develop version.

You can decompress and rename the prepared inference library to fluid_inference and place it in the current directory (`/PATH_TO_PaddleSlim/demo/mkldnn_quant/quant_aware/`). Or by setting PADDLE_ROOT in cmake to specify the location of the Paddle inference library.

#### Compile the application
The directory where the samples are located is `demo/mkldnn_quant/quant_aware/` under PaddleSlim. The samples` sample_tester.cc` and the `cmake` folder required for compilation are all in this directory.

```
cd /PATH/TO/PaddleSlim
cd demo/mkldnn_quant/quant_aware
mkdir build
cd build
cmake -DPADDLE_ROOT=$PADDLE_ROOT ..
make -j
```
If you download and unzip [Inference library from the official website](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/advanced_guide/inference_deployment/inference/build_and_install_lib_cn.html) to the current directory, here`- DPADDLE_ROOT` can not be set, because `DPADDLE_ROOT` default location `demo/mkldnn_quant/quant_aware/fluid_inference`

#### Run the test
```
# Bind threads to cores
export KMP_AFFINITY=granularity=fine,compact,1,0
export KMP_BLOCKTIME=1
# Turbo Boost could be set to OFF using the command
echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
# In the file run.sh, set `MODEL_DIR` to `/PATH/TO/FLOAT32/MODEL` or `/PATH/TO/SAVE/INT8/MODEL`
# In the file run.sh, set `DATA_FILE` to `/PATH/TO/SAVE/BINARY/FILE`
# For 1 thread performance:
./run.sh
# For 20 thread performance:
./run.sh -1 20
```

**Available options in the above command and their descriptions are as follows:**
- **infer_model:** Required. Tested model path. Note that the model parameters files need be saved into multiple files.
- **infer_data:** Required. The path of the tested data file. Note that it needs to be a binary file converted by `full_ILSVRC2012_val_preprocess`.
- **batch_size:** Batch size. The default value is 50.
- **iterations:** Batch iterations. The default is 0, which means predict all batches (image numbers/batch size) in infer_data
- **num_threads:** Number of CPU threads used. The default value is 1.
- **with_accuracy_layer:** If the infer_model require only image input or image input and label input. Default value false.
-**optimize_fp32_model** Whether to optimize the FP32 model. The sample can test INT8 model, or optimize (fuses, etc.) and test FP32 model. The default is false, which means do not do optimization.

You can directly modify MODEL_DIR and DATA_DIR in `run.sh` under `/PATH_TO_PaddleSlim/demo/mkldnn_quant/quant_aware/` directory, then you can execute `./run.sh` for CPU inference.

### 4.3 Users write their own tests:
If the user writes his own test:
1. Test the INT8 model
    If the user tests the converted INT8 model, use paddle::NativeConfig to test. In the demo, set `optimize_fp32_model` to false.
2. Test the FP32 model
   If you want to test the PF32 model, you can use AnalysisConfig to optimize the original FP32 model (fuses, etc.), it will speed up FP32 model performance. AnalysisConfig configuration are as follows:
```
static void SetConfig(paddle::AnalysisConfig *cfg) {
  cfg->SetModel(FLAGS_infer_model); // Required. The model to be tested
  cfg->DisableGpu(); // Required. Deploy on the CPU to predict, you must Disablegpu
  cfg->EnableMKLDNN(); // Required. Configure with MKLDNN enabled make the inference faster than native configuration
  cfg->SwitchIrOptim(); // If the original FP32 model is passed in SetModel, setting SwitchIrOptim to true will fuses many ops hence improve the performance
  cfg->SetCpuMathLibraryNumThreads(FLAGS_num_threads); // The default setting is 1.
}
```
**In the demo we provided**
- If `infer_model` is set FP32 model and `optimize_fp32_model` is set to true, the above AnalysisConfig configuration will be executed automatically. Hence the FP32 model will be fused and optimized, the performance should be faster than FP32 inference using NativeConfig.
- If `infer_model` is converted DNNL INT8 model, `optimize_fp32_model` will make no difference, because INT8 model has been fused, optimized and quantized.
- If `infer_model` is set fake quantized model generated by PaddleSlim, `optimize_fp32_model` does not work even if it is set to true, because the fake quantized model contains fake quantize/dequantize ops, which cannot be fused or optimized.

## 5. Accuracy and performance benchmark
INT8 model accuracy and performance results refer to [CPU deployment predicts the accuracy and performance of INT8 model](https://github.com/PaddlePaddle/PaddleSlim/tree/develop/docs/zh_cn/tutorials/image_classification_mkldnn_quant_aware_tutorial.md)

## FAQ

- For deploying INT8 NLP models on CPU, refer to [ERNIE model quant INT8 accuracy and performance reproduction](https://github.com/PaddlePaddle/benchmark/tree/master/Inference/c%2B%2B /ernie/mkldnn)
- The detailed DNNL quantification process can be viewed in [SLIM quant for INT8 DNNL](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/fluid/contrib/slim/tests/QAT_mkldnn_int8_readme.md)
