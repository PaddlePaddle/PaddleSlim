Distillation:
  alpha: 1.0
  loss: l2
Quantization:
  activation_bits: 8
  is_full_quantize: false
  not_quant_pattern:
  - skip_quant
  quantize_op_types:
  - matmul_v2
  - matmul
  weight_bits: 8
TrainConfig:
  epochs: 1
  eval_iter: 1070
  learning_rate: 2.0e-5
  optimizer_builder:
    optimizer: 
      type: AdamW
    weight_decay: 0.01
  origin_metric: 0.7334
