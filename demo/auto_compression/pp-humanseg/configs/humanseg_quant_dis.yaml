Distillation:
  distill_lambda: 1.0
  distill_loss: l2_loss
  distill_node_pair:
  - teacher_batch_norm_47.tmp_2
  - batch_norm_47.tmp_2
  merge_feed: true
  teacher_model_dir: ./inference_model
  teacher_model_filename: inference.pdmodel
  teacher_params_filename: inference.pdiparams
Quantization:
  activation_bits: 8
  is_full_quantize: false
  not_quant_pattern:
  - skip_quant
  quantize_op_types:
  - conv2d
  - depthwise_conv2d
  weight_bits: 8
TrainConfig:
  epochs: 1
  eval_iter: 400
  learning_rate: 0.0005
  optimizer: SGD
  optim_args:
    weight_decay: 4.0e-05